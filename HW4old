#!/usr/bin/python

import random
import collections
import copy
import math
import sys
import os, random, operator, sys
from collections import Counter

############################################################
# Problem 3: binary classification
############################################################

############################################################
# Problem 3a: feature extraction

def extractWordFeatures(x):
    """
    Extract word features for a string x. Words are delimited by
    whitespace characters only.
    @param string x: 
    @return dict: feature vector representation of x.
    Example: "I am what I am" --> {'I': 2, 'am': 2, 'what': 1}
    """
    
    # BEGIN_YOUR_CODE (our solution is 4 lines of code, but don't worry if you deviate from this)
    xlist = x.split(" ")
    #create a dictionary where for items in list count number of occurances
    FeatureVector = {item: xlist.count(item) for item in xlist}
    return FeatureVector
    #raise Exception("Not implemented yet")
    # END_YOUR_CODE

############################################################
# Problem 3b: stochastic gradient descent

def getMargin(weights, feature, y):
    # y is the label, the value is {-1, 1}.
    return dotProduct(weights, feature) * y

def sparseVectorMultiplication(v, scale) :
    for key in v:
        v[key] = v[key] * scale

def sgd(weights, feature, label, eta):
    # Updates weight.
    # label has value {-1, 1}.
    gradient = collections.defaultdict(float)
    if (1 - getMargin(weights, feature, label)) > 0 :
        gradient = feature
        sparseVectorMultiplication(gradient, -label)
    else:
        # gradient is all 0 in this case.
        pass

    increment(weights, (-1) * eta, gradient)

def getPredictor(weights, featureExtractor):
    # A linear -1/1 predictor, the decision boundary is 0.
    return lambda x : 1 if dotProduct(weights, featureExtractor(x)) >= 0 else -1

def learnPredictor(trainExamples, testExamples, featureExtractor, numIters, eta):
    '''
    Given |trainExamples| and |testExamples| (each one is a list of (x,y)
    pairs), a |featureExtractor| to apply to x, and the number of iterations to
    train |numIters|, the step size |eta|, return the weight vector (sparse
    feature vector) learned.

    You should implement stochastic gradient descent.

    Note: only use the trainExamples for training!
    You should call evaluatePredictor() once each epoch on both trainExamples and testExamples
    to see how you're doing as you learn after each iteration.
    '''
    weights = {}  # feature => weight
    #sgd(weights, feature, label, eta):
    numUpdates = 0
    #feature = featureExtractor()
    #numIters is number of epochs/number of times to train
    for epoch in range(numIters):
        #for x, y in trainExamples:
        x = trainExamples.keys()
        feature = featureExtractor(x)
        y = trainExamples.values()
        label = y
        sgd(weights, feature, label, eta)
    #evaluatePredictor(trainExamples,)   
    
    # BEGIN_YOUR_CODE (our solution is 12 lines of code, but don't worry if you deviate from this)
    raise Exception("Not implemented yet")
    # END_YOUR_CODE
    return weights

############################################################
# Problem 3c: generate test case

def generateDataset(numExamples, weights):
    '''
    Return a set of examples (phi(x), y) randomly which are classified correctly by
    |weights|.
    '''
    random.seed(42)
    # Return a single example (phi(x), y).
    # phi(x) should be a dict whose keys are a subset of the keys in weights
    # and values can be anything (randomize!) with a nonzero score under the given weight vector.
    # y should be 1 or -1 as classified by the weight vector.
    def generateExample():
        # BEGIN_YOUR_CODE (our solution is 3 lines of code, but don't worry if you deviate from this)    
        raise Exception("Not implemented yet")
        # END_YOUR_CODE
        return (phi, y)

    return [generateExample() for _ in range(numExamples)]

############################################################
# Problem 3e: character features

def extractCharacterFeatures(n):
    '''
    Return a function that takes a string |x| and returns a sparse feature
    vector consisting of all n-grams of |x| without spaces.
    EXAMPLE: (n = 3) "I like tacos" --> {'Ili': 1, 'lik': 1, 'ike': 1, ...
    You may assume that n >= 1.
    '''
    def extract(x):
        # BEGIN_YOUR_CODE (our solution is 6 lines of code, but don't worry if you deviate from this)
        raise Exception("Not implemented yet")
        # END_YOUR_CODE
    return extract

############################################################
# Problem 4: k-means
############################################################

def initListOfDict(size):
    rtn = list()
    for i in range(0,size):
        rtn.append(collections.defaultdict(float))
    return rtn

# An optimization on getting distance, centroid is dense, while dataPoint is sparse.
def getDistSqr(dataPoint, centroids, centroidSqrSums, centroidIndex):
    rtn = centroidSqrSums[centroidIndex]
    for k, v in dataPoint.items():
        rtn += v * v - 2 * centroids[centroidIndex].get(k, 0) * v
    return rtn

def kmeans(examples, K, maxIters):
    '''
    examples: list of examples, each example is a string-to-double dict representing a sparse vector.
    K: number of desired clusters. Assume that 0 < K <= |examples|.
    maxIters: maximum number of iterations to run (you should terminate early if the algorithm converges).
    Return: (length K list of cluster centroids,
            list of assignments (i.e. if examples[i] belongs to centers[j], then assignments[i] = j)
            final reconstruction loss)
    '''
    # BEGIN_YOUR_CODE (our solution is 32 lines of code, but don't worry if you deviate from this)
    raise Exception("Not implemented yet")
    # END_YOUR_CODE





### util ###

def dotProduct(d1, d2):
    """
    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).
    @param dict d2: same as d1
    @return float: the dot product between d1 and d2
    """
    if len(d1) < len(d2):
        return dotProduct(d2, d1)
    else:
        return sum(d1.get(f, 0) * v for f, v in d2.items())

def increment(d1, scale, d2):
    """
    Implements d1 += scale * d2 for sparse vectors.
    @param dict d1: the feature vector which is mutated.
    @param float scale
    @param dict d2: a feature vector.
    """
    for f, v in d2.items():
        d1[f] = d1.get(f, 0) + v * scale

def readExamples(path):
    '''
    Reads a set of training examples.
    '''
    examples = []
    for line in open(path):
        # Format of each line: <output label (+1 or -1)> <input sentence>
        y, x = line.split(' ', 1)
        examples.append((x.strip(), int(y)))
    print( 'Read %d examples from %s' % (len(examples), path))
    return examples

def evaluatePredictor(examples, predictor):
    '''
    predictor: a function that takes an x and returns a predicted y.
    Given a list of examples (x, y), makes predictions based on |predict| and returns the fraction
    of misclassiied examples.
    '''
    error = 0
    for x, y in examples:
        if predictor(x) != y:
            error += 1
    return 1.0 * error / len(examples)

def outputErrorAnalysis(examples, featureExtractor, weights, path):
    out = open('error-analysis', 'w')
    for x, y in examples:
        out.write('===', x)
        verbosePredict(featureExtractor(x), y, weights, out)
    out.close()

def interactivePrompt(featureExtractor, weights):
    while True:
        print ('> ',)
        x = sys.stdin.readline()
        if not x: break
        phi = featureExtractor(x) 
        verbosePredict(phi, None, weights, sys.stdout)

############################################################

# def generateClusteringExamples(numExamples, numWordsPerTopic, numFillerWords):
#     '''
#     Generate artificial examples inspired by sentiment for clustering.
#     Each review has a hidden sentiment (positive or negative) and a topic (plot, acting, or music).
#     The actual review consists of 2 sentiment words, 4 topic words and 2 filler words, for example:

#         good:1 great:1 plot1:2 plot7:1 plot9:1 filler0:1 filler10:1

#     numExamples: Number of examples to generate
#     numWordsPerTopic: Number of words per topic (e.g., plot0, plot1, ...)
#     numFillerWords: Number of words per filler (e.g., filler0, filler1, ...)
#     '''
#     sentiments = [['bad', 'awful', 'worst', 'terrible'], ['good', 'great', 'fantastic', 'excellent']]
#     topics = ['plot', 'acting', 'music']
#     def generateExample():
#         x = Counter()
#         # Choose 2 sentiment words according to some sentiment
#         sentimentWords = random.choice(sentiments)
#         x[random.choice(sentimentWords)] += 1
#         x[random.choice(sentimentWords)] += 1
#         # Choose 4 topic words from a fixed topic
#         topic = random.choice(topics)
#         x[topic + str(random.randint(0, numWordsPerTopic-1))] += 1
#         x[topic + str(random.randint(0, numWordsPerTopic-1))] += 1
#         x[topic + str(random.randint(0, numWordsPerTopic-1))] += 1
#         x[topic + str(random.randint(0, numWordsPerTopic-1))] += 1
#         # Choose 2 filler words
#         x['filler' + str(random.randint(0, numFillerWords-1))] += 1
#         return x

#     random.seed(42)
#     examples = [generateExample() for _ in range(numExamples)]
#     return examples


